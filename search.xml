<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Parameter Tuning in DNN]]></title>
    <url>%2Fnote%2F2019-07-28-parameter-tuning-in-dnn%2F</url>
    <content type="text"><![CDATA[I built a simple fitting model with some hidden layers via tensorflow and recorded some experiment results in this blog post. The data is generated as $$y = \sin(2x) + uniform(0.0, 1e-1)$$ You can check the code in Github Gist . ParametersThere are some hyper parameters being used: DATA_NUMBERS: how many data will be generated HIDDEN_LAYER_NUM: how many hidden layers in the model HIDDEN_STATES: how many hidden states in each hidden layer DROPOUT_RATE: dropout rate (to prevent overfitting) LEARNING_RATE: learning rate of optimizers OPTIMIZER: which optimizer are we going to use, Adam, GD, Adadelta and Adagrad are supported EPOCH: how many epochs this model is going to “learn” STOP_NUM: how many epochs are we going to stop the training when the loss almost not changes Control ExperimentDoes Data Points Counts?Parameters HIDDEN_LAYER_NUM HIDDEN_STATES DROPOUT_RATE LEARNING_RATE OPTIMIZER EPOCH STOP_NUM 2 100 0.0 1e-3 Adam 2000 200 *STOP_EPOCH means how many epochs does the model stop training when STOP_NUM does not greater equal than EPOCH. Results DATA_NUMBERS STOP_EPOCH LOSS TIME(s) 5 330 0.000011 19.284 10 563 0.106048 32.709 20 569 0.085790 32.886 50 555 0.067058 31.869 100 551 0.060067 31.719 200 549 0.059315 31.697 500 545 0.057501 31.769 1000 549 0.055958 32.736 2000 545 0.057463 35.700 4000 549 0.055416 37.032 ConclusionWe can see that, the data points factor slightly affects the training time when the amount is very small. But when it comes to larger dataset, the effects becomes less important. Does Hidden Layer Number Counts?Parameters DATA_NUMBERS HIDDEN_STATES DROPOUT_RATE LEARNING_RATE OPTIMIZER EPOCH STOP_NUM 4000 100 0.0 1e-3 Adam 2000 200 Results HIDDEN_LAYER_NUM STOP_EPOCH LOSS TIME(s) 1 1152 0.434874 75.278 2 549 0.055416 37.935 3 561 0.001136 41.058 4 407 0.000836 29.272 5 1702 0.000829 127.528 10 2000 0.000851 169.560 20 2000 0.000832 214.177 50 549 0.055958 32.736 ConclusionFitting with one hidden layer: One hidden layer is too simple to represent complex funcitions. Fitting with 10 hidden layers: 10 hidden layers are super powerful for such a simple fitting problem, and the model coverges at about 200 epochs. Maybe the learning rate is a little high, so in the latter epochs, the fitting curve oscillates and not stopped until full 2000 epochs finished. Fitting with 50 hidden layers: 50 hidden layers are too complex to train, so as time goes by, there is too much updates working to be done. 2000 epoch is too short for such a complex model to train. So when it comes to meet the STOP_NUM, the program thinks the model has coverged, the process then stops. The hidden layer is very important for fitting. Hidden layer represents an ability to solve more complex problems. But, the more hidden layers you add, the model is more willing to be overfitted and get more complex, especially in some simple problems. When the hidden layer number becomes very large (e.g. 50), the model is too hard to learn. So stacking hidden layers without thinking cannot solve problems but a waste of time and computing resource. Does Hidden States Counts?Parameters DATA_NUMBERS HIDDEN_LAYER_NUM DROPOUT_RATE LEARNING_RATE OPTIMIZER EPOCH STOP_NUM 1000 4 0.0 1e-3 Adam 2000 200 Results HIDDEN_STATES STOP_EPOCH LOSS TIME(s) 1 256 0.498747 15.813 20 519 0.032542 32.252 50 579 0.000945 35.670 100 532 0.000780 33.675 200 2000 0.000777 135.134 500 2000 0.000851 169.560 1000 2000 0.000855 163.531 ConclusionFitting with 1 hidden state: If the number of hidden state is too small, the model will learn in a slow way (but less parameters). When it becomes much more larger, the fitting curve would like to oscillate (a more complex model needs a smaller learning rate). Does Dropout Rate Counts?Parameters DATA_NUMBERS HIDDEN_LAYER_NUM HIDDEN_STATES LEARNING_RATE OPTIMIZER EPOCH STOP_NUM 15 4 100 1e-3 Adam 2000 200 ResultsDropout on each hidden layer: DROPOUT_RATE STOP_EPOCH LOSS TIME(s) 0.01 2000 0.001431 125.321 0.02 2000 0.003937 119.524 0.05 2000 0.014249 121.020 0.1 2000 0.028546 118.456 0.2 2000 0.093015 120.563 0.5 2000 0.331162 119.405 0.9 2000 43.029678 116.994 ConclusionNo dropout: 0.01 dropout: 0.5 dropout: 0.9 dropout: The bigger dropout rate you set, the more oscillating loss curve you will get. But in a more complex problem, dropout can prevent overfitting. Dropout is a mechanism to drop some nerons to prevent parameters overfitting in forward propogation. In some simple models, dropout has a more probability to drop some useful information. What will dropout do in a deep model ? DATA_NUMBERS HIDDEN_LAYER_NUM HIDDEN_STATES DROPOUT_RATE LEARNING_RATE OPTIMIZER EPOCH STOP_NUM 15 20 1000 0.05 1e-3 Adam 2000 200 Not overfitting, ha. What if the learning rate is lower? DATA_NUMBERS HIDDEN_LAYER_NUM HIDDEN_STATES DROPOUT_RATE LEARNING_RATE OPTIMIZER EPOCH STOP_NUM 15 20 1000 0.05 1e-5 Adam 2000 200 From the figure above we can see that, the smaller learning rate just affects the coverge speed. Does Learning Rate Counts?Parameters DATA_NUMBERS HIDDEN_LAYER_NUM HIDDEN_STATES DROPOUT_RATE OPTIMIZER EPOCH STOP_NUM 1000 4 100 0.0 Adam 2000 200 Results LEARNING_RATE STOP_EPOCH LOSS TIME(s) 1e-9 201 3.471233 13.058 1e-5 2000 0.359172 125.924 1e-3 532 0.000780 33.526 0.1 2000 0.015864 125.858 0.3 1337 0.501801 84.967 0.5 2000 1.051182 123.813 0.9 2000 12.042709 125.234 Conclusionlearning rate = 1e-9: learning rate = 1e-5: learning rate = 1e-3: From the above figures we can get: a larger learning rate will push the training to become more faster. But a too large value will make the model not able to corvenge. A larger and not suitable loss curve may oscillate like this: Does Optimizer Counts?Parameters DATA_NUMBERS HIDDEN_LAYER_NUM HIDDEN_STATES DROPOUT_RATE LEARNING_RATE EPOCH STOP_NUM 1000 4 100 0.0 1e-3 2000 200 Results OPTIMIZER STOP_EPOCH LOSS TIME(s) GradientDescent 520 0.405575 32.961 Adam 532 0.000780 33.837 Adadelta 2000 1.178411 127.297 Adagrad 2000 0.163867 126.752 ConclusionThe table shows that the Gradient Descent takes less time to train. However, it has a higher loss value than Adam optimizer. In most cases, Adam optimizer is faster and better than naive Gradient Descent, especially when models are very complex. Does Activation Function Counts?Parameters DATA_NUMBERS HIDDEN_LAYER_NUM HIDDEN_STATES DROPOUT_RATE LEARNING_RATE OPTIMIZER EPOCH STOP_NUM 1000 4 100 0.0 1e-3 Adam 2000 200 Results ACTIVATION_FUNCTION STOP_EPOCH LOSS TIME(s) tf.nn.relu 532 0.000780 34.269 tf.nn.sigmoid 444 0.425101 27.606 tf.nn.tanh 739 0.003126 47.672 tf.nn.leaky_relu 595 0.000812 37.365 ConclusionLeaky relu can solve gradient vanishing problems in some ways. So I set HIDDEN_LAYER_NUM to 50 and make a test again. Here is the result: The model is still not trained well in a smart way, but there are some ripples on the loss curve. So I have the model to train the whole 2000 epochs. And the result shows this does not make sense. Interesting…]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>DNN</tag>
        <tag>tuning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Add Images into Hexo]]></title>
    <url>%2Fnote%2F2019-07-26-add-image-in-hexo%2F</url>
    <content type="text"><![CDATA[Hexo is not very image friendly. So we may not insert local images just like naive markdown. We should install a plugin 12npm install https://github.com/CodeFalling/hexo-asset-image -- save# or npm install https://github.com/7ym0n/hexo-asset-image --save Set _config.yml 1post_asset_folder: true When we create a new post, hexo will generate a folder with the same name. We can put images or other things into this folder and add a tag like below into the post. 1&#123;% asset_img page2.png page2 %&#125; Or just insert images like ![alt](page1.png). 123hexo cleanhexo ghexo s And finally, you could see these pictures in the local server.]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Derivation of Cross Entropy with Softmax]]></title>
    <url>%2Fnote%2F2019-07-26-softmax-crossentropy%2F</url>
    <content type="text"><![CDATA[Derivation of Cross Entropy with Softmax]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>math</tag>
        <tag>derivative</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to Start Hexo]]></title>
    <url>%2Fnote%2F2019-02-13-How-to-Start-Hexo%2F</url>
    <content type="text"><![CDATA[Hexo is a static blog framework and we could use markdown syntax to render pages. InstallationFirst, we have to get Node.js and Git ready. Here are the two links: Node.js Git Second, we should install hexo framework through npm commands. 12345$ npm install hexo-cli -g$ hexo init blog$ cd blog$ npm install$ hexo server Third, I also use a third-party theme, the NexT. The Hexo version is 3.8.0 and the NexT version is 7.0.0 . ConfigurationThe global configuration file: _config.yml123456789101112language: zh-CNurl: https://spico197.github.iopermalink: :category/:year-:month-:day-:title.htmltheme: nextsearch: path: search.xml field: post format: html limit: 10000deploy: type: git repo: git@github.com:Spico197/Spico197.github.io.git Here, I use the Git deployment method and I use a ssh-sync repo. The theme config file in theme/next/_config.yml: 123456789101112131415161718192021222324252627282930313233343536373839# 访客数和访问数统计busuanzi_count: enable: true total_visitors: true total_visitors_icon: user total_views: true total_views_icon: eye post_views: false post_views_icon: eye# 本地搜索功能local_search: enable: true# 网页加载进度条pace: true# 允许加载数学公式math: enable: true# 首页不显示文章全部内容auto_excerpt: enable: true# 侧边栏始终显示sidebar: display: always# 设置头像图片avatar: url: /images/avatar.jpg# 设置主题模式scheme: Gemini# 设置菜单menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive# 设置社交联系方式social: GitHub: https://github.com/Spico197 || github E-Mail: mailto:spico1026@gmail.com || envelope Writing and DeploymentCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment References Hexo-Github搭建自己的博客 — 心得汇总2018版（主题配置篇） Hexo+Github搭建自己的博客 — 心得汇总2018版（内容编辑篇）]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
